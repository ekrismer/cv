{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from enum import Enum\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Used Dataset\n",
    "\n",
    "We used the dataset CIFAR10 for training and testing our neural networks. The dataset contains a total of 60000 colour images, which are split into 50000 training images and 10000 test images. Each image has a resolution of 32x32 pixel. \n",
    "\n",
    "As the name suggests the CIFAR-10 dataset consists of 10 classes, with 6000 images each.\n",
    "The dataset is further split into 5 training batches รก 10000 images. Each training batch can have a different amount of pictures from the ten classes, whereas the test batch contains exactly 1000 pictures of each class.\n",
    "\n",
    "The ten classes are:  \n",
    "    * airplane\n",
    "    * automobile\n",
    "    * bird\n",
    "    * cat\n",
    "    * deer\n",
    "    * dog\n",
    "    * frog\n",
    "    * horse\n",
    "    * ship\n",
    "    * truck\n",
    "    \n",
    "The following figure shows 10 sample images for each class:\n",
    "<img src=\"CIFAR-10-example.PNG\" width=\"450px\" alt=\"CIFAR-10 example\" title=\"CIFAR-10 example images\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Torchvision datasets are _PILImage_ images in range [0, 1] we  have to transform them to normalized Tensors in range [-1, 1].\n",
    "\n",
    "To diversify the network input, the training images are additionally augmented by adding a 4 pixel padding on each side, cropping them back to 32x32 pixels and the randomly flip them horizontally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data zero-padded by 4 pixels on each side, randomly cropped to a 32x32 image and eventually flipped horizontally\n",
    "# zero padding according to reference [24] - https://arxiv.org/pdf/1409.5185.pdf\n",
    "tf_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(size=32, padding=4),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # image = (image - mean) / std\n",
    "     ])\n",
    "\n",
    "# test data is simply normalized\n",
    "tf_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # image = (image - mean) / std\n",
    "     ])\n",
    "\n",
    "## ---------------------------------------------- ##\n",
    "#select network to be used for training\n",
    "class NeuralNet(Enum):\n",
    "    Net = 1\n",
    "    ResNet20 = 2\n",
    "    ResNet32 = 3\n",
    "    \n",
    "network = NeuralNet.Net\n",
    "\n",
    "# in case of a residual network this flag allows to remove the shurtcut connection, making it a plain network\n",
    "use_shortcut = True # True = residual // False = plain \n",
    "\n",
    "examplesCnt = 8 # how many sample images with ground truth/prediction shall be printed?\n",
    "\n",
    "batchSize = 200 # utilize parallel processing power of GPU, further increasing this number doesn't seem to have any effect\n",
    "## ---------------------------------------------- ##\n",
    "\n",
    "# define the data loaders for our training and test images\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tf_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tf_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize, shuffle=False, num_workers=4)\n",
    "\n",
    "# labels of the ten classes ussed for evaluation\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "images, labels = iter(trainloader).next()\n",
    "images = images[0:examplesCnt]\n",
    "labels = labels[0:examplesCnt]\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(', '.join('%5s' % classes[labels[j]] for j in range(examplesCnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural networks:\n",
    "\n",
    "We decided to use 3 different network architectures and compare the results each network gives.\n",
    "\n",
    "<b> Convulutional neural network </b>\n",
    "As a starter we used the basic convolutional neural network from PyTorch (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). We quickly realized that accuracy and training speed were far from optimal and started tweaking the parameters.\n",
    "In order to increase speed we raised the batch-size from originally 4 to 200. We stopped at 200 because there was no further improvment regarding the computation time.\n",
    "In order to adapt to the new batch-size we had to increase our learning rate (originally 0.001). Since the optimizer now has more images for computing the gradient. Hence it's a better approximation needs a bigger learning rate in order to efficiently improve the result. Without a larger learning rate we would need way more epoches to compensate the more precise gradient. \n",
    "\n",
    "Additional we decided to increase the input and output layers for the convolutions. Our thoughtprocess behind this, was to increase accuracy. Looking at more features from a picture should increase learning accuracy and lead to a better result. We tried with increasing inputplanes and stopped once we stopped noticing improvment in accuracy.\n",
    "Hence the following 3 computations changed:\n",
    "\n",
    "<b>Network definition:</b>  \n",
    "self.conv1 = nn.Conv2d(3, 6, 5) -> self.conv1 = nn.Conv2d(3, 48, 5)  \n",
    "self.conv2 = nn.Conv2d(6, 16, 5) -> self.conv2 = nn.Conv2d(48, 64, 5)\n",
    "\n",
    "<b>Forward-Step</b> (This change was necessary for the network to work, not for improvment. Otherwise the dimensions for the function would not match)  \n",
    "x = x.view(-1, 16 * 5 * 5) -> x = x.view(-1, 64 * 5 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, \n",
    "        out_planes, \n",
    "        kernel_size=1, \n",
    "        stride=stride, \n",
    "        #padding=0\n",
    "        bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, \n",
    "                     out_planes, \n",
    "                     kernel_size=3, \n",
    "                     stride=stride,\n",
    "                     padding=1, # with stride=1 this preserves dimension\n",
    "                     bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Conv2d: input, output, kernel size, (stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 48, 5) # 3 input image channel, 64 output channels, 5x5 square convolution\n",
    "        self.conv2 = nn.Conv2d(48, 64, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # 10 classses output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # 48 channels: conv results in 28x28 (image-kernel+1), pool => 14x14 image size\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x))) # 64 channels: conv results in 10x10 (image-kernel+1), pool => 5x5 image size\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Implementations\n",
    "\n",
    "We choose two Resnet Implementations, ResNet-34 and Resnet-50. Since both use different Blocks \n",
    "(Resnet-34 uses BasicBlocks, Resnet-50 usese Bottleneck Blocks). Theses networks are not too far apart, regarding layer \n",
    "size, but use different kind of Blocks, so we were interested to see how this difference would impact accuracy and runtime.\n",
    "\n",
    "### Resnet-34 BasisBlock\n",
    "\n",
    "For the BasicBlock Implementation we used the lecture as a template.  \n",
    "\n",
    "![BasicBlock](BasicBlock.png)\n",
    "\n",
    "### Resnet-34\n",
    "\n",
    "For the the Resnet-34 we tried to implement the network while using the paper as a model. We noticed, that the implementation from our paper is very similar to the one pytorch(same holds for Resnet-50). \n",
    "    * Each resnet useses either Basic- or BottleneckBlocks, but never both\n",
    "    * Both use the same data processing technique (# TODO state these techniques)\n",
    "    * Both use a Linear network in order to map the results to our ten classes\n",
    "    \n",
    "In the paper way more epoches are used than we could handle, since we are missing the computational power. Hence we tried to adjust the learning rate in order to achieve a fast but still accurate result. \n",
    "Tweaks we did #TODO\n",
    "\n",
    "### Resnet-50 BottleneckBlock\n",
    "\n",
    "For the BottleneckBlock we also used the template from the lecture.  \n",
    "\n",
    "![BotleneckBlock](BottleneckBlock.png)\n",
    "\n",
    "#### Resnet-50 \n",
    "\n",
    "Tweaks we did #TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        3x3 convolution inplanes->outplanes \n",
    "        (spatial size maintained), BN + ReLU\n",
    "        \"\"\" \n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        3x3 convolution inplanes->outplanes \n",
    "        (spatial size maintained) + BN\n",
    "        \"\"\" \n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # needed to fix dimension incompatibility\n",
    "        self.stride = stride # not actually used in class itself\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x # preserve input for identity functionality\n",
    "\n",
    "        # conv->bn->relu\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # conv->bn\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        \"\"\"\n",
    "        At that point, we check if a downsampling needs to be \n",
    "        applied on x, such that F(x)+x can be computed.\n",
    "        \"\"\"\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        # shortcut connection: x + F(x)\n",
    "        if use_shortcut:\n",
    "            out += residual\n",
    "        \n",
    "        # building block ends with ReLU\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cuda device if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet20(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet20, self).__init__()\n",
    "        \n",
    "        self.inplanes = 16\n",
    "        self.conv1 = conv3x3(3, self.inplanes, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #First layerblock\n",
    "        planes = 16\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer1 = nn.Sequential(*layers)\n",
    "        \n",
    "        #Second layerblock\n",
    "        planes = 32\n",
    "        layers = []\n",
    "        down_fn = nn.Sequential(conv1x1(self.inplanes, planes * BasicBlock.expansion, stride=2), nn.BatchNorm2d(planes * BasicBlock.expansion) )\n",
    "        layers.append(BasicBlock(self.inplanes, planes, 2, down_fn))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer2 = nn.Sequential(*layers)\n",
    "        \n",
    "        # Third layerblock\n",
    "        planes = 64\n",
    "        layers = []\n",
    "        down_fn = nn.Sequential(conv1x1(self.inplanes, planes * BasicBlock.expansion, stride=2), nn.BatchNorm2d(planes * BasicBlock.expansion) )\n",
    "        layers.append(BasicBlock(self.inplanes, planes, 2, down_fn))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer3 = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # Pooling is not needed, because we don't need to downsample, our desired input size is 32 which we aready have\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet32(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet32, self).__init__()\n",
    "        \n",
    "        self.inplanes = 16\n",
    "        self.conv1 = conv3x3(3, self.inplanes, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #First layerblock\n",
    "        planes = 16\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer1 = nn.Sequential(*layers)\n",
    "        \n",
    "        #Second layerblock\n",
    "        planes = 32\n",
    "        layers = []\n",
    "        down_fn = nn.Sequential(conv1x1(self.inplanes, planes * BasicBlock.expansion, stride=2), nn.BatchNorm2d(planes * BasicBlock.expansion) )\n",
    "        layers.append(BasicBlock(self.inplanes, planes, 2, down_fn))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer2 = nn.Sequential(*layers)\n",
    "        \n",
    "        # Third layerblock\n",
    "        planes = 64\n",
    "        layers = []\n",
    "        down_fn = nn.Sequential(conv1x1(self.inplanes, planes * BasicBlock.expansion, stride=2), nn.BatchNorm2d(planes * BasicBlock.expansion) )\n",
    "        layers.append(BasicBlock(self.inplanes, planes, 2, down_fn))\n",
    "        self.inplanes = planes * BasicBlock.expansion\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        layers.append(BasicBlock(self.inplanes, planes))\n",
    "        self.layer3 = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                n.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # Pooling is not needed, because we don't need to downsample, our desired input size is 32 which we aready have\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which network should be used:\n",
    "# every training epoch has 250 iterations with each 200 images\n",
    "if network == NeuralNet.Net:\n",
    "    net = Net()\n",
    "    epochCnt = 40\n",
    "elif network == NeuralNet.ResNet20:\n",
    "    net = ResNet20()\n",
    "    epochCnt = 10\n",
    "elif network == NeuralNet.ResNet32:\n",
    "    net = ResNet32()\n",
    "    epochCnt = 10\n",
    "else:\n",
    "    sys.exit(\"invalid network type!\")\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "# print number of paramters in currently selected network\n",
    "netParamCnt = 0\n",
    "for p in net.parameters(): netParamCnt+=p.numel()\n",
    "print(\"%s has %d parameters.\" % (network.name, netParamCnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Loss function and optimizer:\n",
    "\n",
    "Additional to the optimizer we used a scheduler to adapt our learingn rate. In the paper the same technique is used in order to achieve better results. As a consequence we needed to increase our epoches, so the scheduler could work efficiently (decrease learning rate at least two times). This also lead to an increased amount of time our network needed for the learning proccess.\n",
    "\n",
    "For the loss we used a Classification Cross-Entropy loss and SGD with momentum. This is the same in pytorch as well as our paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8000, 12000], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, trainloader, optimizer, epoch):\n",
    "    model.train() # set model to training mode\n",
    "    train_loss = 0\n",
    "    accuracy_per_batch = []\n",
    "    \n",
    "    # iterate over all batches of (img, label) from trainloader\n",
    "    # images are augmented randomly by specified transformers\n",
    "    for batch_idx, (data, target) in enumerate(trainloader, 1): # 50.000 test images => 250 iterations with each 200 images\n",
    "        scheduler.step()\n",
    "        \n",
    "        # use CUDA if available\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # reset parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass \n",
    "        output = net(data) # 10 class output\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update paramters\n",
    "        optimizer.step()\n",
    "\n",
    "        # some logging\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correctCurrentBatch = pred.eq(target.view_as(pred)).sum().item()\n",
    "        accuracy_per_batch.append(100. * correctCurrentBatch / len(data))\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 25 == 0:    # print every tenth iteration\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),    # number of images processed so far\n",
    "                len(trainloader.dataset), # total number of images in trainingset\n",
    "                100. * batch_idx / len(trainloader), # progress in percent\n",
    "                loss.item())\n",
    "            )\n",
    "\n",
    "    train_loss /= len(trainloader)\n",
    "    accuracy_per_epoch = sum(accuracy_per_batch)/len(accuracy_per_batch) #average out detailed values for further processing\n",
    "    return (train_loss, accuracy_per_batch, accuracy_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, testloader):\n",
    "    model.eval() # set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    accuracy_per_batch = []\n",
    "    \n",
    "    # temporarily set all the requires_grad flag to False\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # iterate over testing batches  (img, label)\n",
    "        for data, target in testloader:\n",
    "            \n",
    "            # use CUDA if available\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # forward pass \n",
    "            output = net(data)\n",
    "            \n",
    "            # track cross-entropy-loss on testing data\n",
    "            test_loss += criterion(output, target).item() \n",
    "            \n",
    "            # get the correct class prediction as the max. value of the last fully-connected layer\n",
    "            # Remember, the cross-entropy-loss operates on top of that layer! In that sense, the \n",
    "            # output of the last layer can be interpreted as the log-probability of each class.\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            \n",
    "            # Track how many labels were correctly predicted\n",
    "            correctCurrentBatch = pred.eq(target.view_as(pred)).sum().item()\n",
    "            accuracy_per_batch.append(100. * correctCurrentBatch / len(data))\n",
    "            correct += correctCurrentBatch\n",
    "\n",
    "    # Overall testing loss\n",
    "    test_loss /= len(testloader)\n",
    "    accuracy_per_epoch = 100. * correct / len(testloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss,\n",
    "        correct,\n",
    "        len(testloader.dataset),\n",
    "        accuracy_per_epoch)\n",
    "    )\n",
    "\n",
    "    return (test_loss, accuracy_per_batch, accuracy_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, iterate over a couple of epochs ...\n",
    "train_losses = []\n",
    "train_accuracies_iter = []\n",
    "train_accuracies_epoc = []\n",
    "test_losses  = []\n",
    "test_accuracies_iter = []\n",
    "test_accuracies_epoc = []\n",
    "\n",
    "# measure the time the training takes\n",
    "time_start = time.process_time()\n",
    "\n",
    "for epoch in range(1, epochCnt+1):\n",
    "    train_loss, train_acc_iter, train_acc_epoc = train(net, device, trainloader, optimizer, epoch)\n",
    "    test_loss, test_acc_iter, test_acc_epoc  = test(net, device, testloader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies_iter.extend(train_acc_iter)\n",
    "    train_accuracies_epoc.append(train_acc_epoc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies_iter.extend(test_acc_iter)\n",
    "    test_accuracies_epoc.append(test_acc_epoc)\n",
    "\n",
    "\n",
    "print('Finished Training (duration %.3f sec)' % (time.process_time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(trainloader), (epochCnt+1)*len(trainloader), len(trainloader))\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x, train_losses, lw=1.5)\n",
    "plt.plot(x, test_losses, lw=1.5)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss (average per epoch)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.grid()\n",
    "plt.savefig('results/' + network.name + '_epochs_' + str(epochCnt) + '_loss.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# we have one accuracy value per iteration\n",
    "#  => training: 50000/200 = 250 iterations per epoch\n",
    "#  => testing:  10000/200 =  50 iterations per epoch\n",
    "# => only use every nth element of list (alternatively one could implement a moving average or similar)\n",
    "train_accuracies_reduced = train_accuracies_iter[0::len(trainloader)]\n",
    "test_accuracies_reduced = test_accuracies_iter[0::len(testloader)]\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x, 100-np.array(train_accuracies_reduced), lw=1.5)\n",
    "plt.plot(x, 100-np.array(test_accuracies_reduced), lw=1.5)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Error [%]')\n",
    "plt.title('Error (after each epoch)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.ylim((0,45))\n",
    "plt.grid()\n",
    "plt.savefig('results/' + network.name + '_epochs_' + str(epochCnt) + '_error.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x, 100-np.array(train_accuracies_epoc), lw=1.5)\n",
    "plt.plot(x, 100-np.array(test_accuracies_epoc), lw=1.5)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Error [%]')\n",
    "plt.title('Error (average per epoch)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.ylim((0,45))\n",
    "plt.grid()\n",
    "plt.savefig('results/' + network.name + '_epochs_' + str(epochCnt) + '_error_avg.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network\n",
    "\n",
    "Original we used two epoches for the non-residual network. While using the Res-Net we noticed that 2 epoches were not enough especially when using a scheduler for the learning rate. We decided to use 5 epoch in order to get at least two learning-rate reductions. This should be enough since we did not notice any additional change to our loss. Especially for lower learning rates.\n",
    "\n",
    "In order to check the results our networks have achieved we decided to use the template from pytorch and the lecture, splitting in in 3 parts:\n",
    "    * Check 8 sample images and their computed labels\n",
    "    * Total Accuracy\n",
    "    * Class Accuracy\n",
    "\n",
    "<b>Sample Images with computed labels:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images = images[0:examplesCnt]\n",
    "labels = labels[0:examplesCnt]\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ', '.join('%5s' % classes[labels[j]] for j in range(examplesCnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predicted Classes from our network:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move input to cuda device if available\n",
    "images = images.to(device)\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ', '.join('%5s' % classes[predicted[j]] for j in range(examplesCnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Total Accuracy:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = net(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %2d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Class Accuracy:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = net(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == target).squeeze()\n",
    "        for i in range(batchSize):\n",
    "            label = target[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s: %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
